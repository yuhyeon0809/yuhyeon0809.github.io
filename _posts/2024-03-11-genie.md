---
title: "Genie: Generative Interactive Environments 리뷰"
excerpt: "구글 딥마인드의 Foundation World Model, Genie 논문을 리뷰합니다."

categories: 
  - paper-review
# tags: 
#   - [tag1, tag2]

permalink: /paper-review/genie/ 

toc: true
toc_sticky: true
---

[https://arxiv.org/abs/2402.15391](https://arxiv.org/abs/2402.15391){:target="_blank"}  
[https://sites.google.com/view/genie-2024](https://sites.google.com/view/genie-2024){:target="_blank"}  

<img width="776" alt="genie_main" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/71f89e14-86a6-49b9-bb20-325d8aeb03b8">

## Intro

<!-- 간략한 소개 -->
지난 2월, Google Deepmind가 공개한 생성적 상호작용 환경 (Generative Interactive Environments) 모델, **Genie🧞** 입니다. Text-to-Image 모델로 생성한 이미지, 손으로 그린 스케치, 실제 사진을 입력으로 받아 액션 제어가 가능한 게임 세계(Playable Worlds)를 생성하는 모델입니다. 

<!-- 등장 배경 -->
지난 몇 년간 생성 AI의 엄청난 발전이 있었음에도, ChatGPT와 같은 언어 모델과 영상 생성 모델간의 사용자 상호작용 및 참여 수준의 격차는 여전히 존재합니다. 유저가 프롬프트를 입력하면 모델이 생성 결과를 출력하고, 그 결과를 바탕으로 유저가 또 다시 프롬프트를 입력하는 식의, 소위 '티키타카'라고 하는 상호작용이 생성 AI 사용에 있어 더욱 몰입감 있는 경험을 만들어내는 것도 사실입니다. 

*만일, 인터넷의 방대한 비디오 데이터가 주어진다면?*

새로운 이미지 및 비디오 생성 뿐만 아니라, 완전히 인터랙티브한 경험(Entire Interactive Experience)을 만들어낼 수 있지 않을까요? 이러한 맥락에서, 구글 딥마인드는 생성 AI의 새로운 패러다임, Generative Interactive Environments를 제안합니다. 하나의 텍스트/이미지 프롬프트로부터 인터랙티브한 '환경'을 생성하는 것이죠. 

<!-- 핵심 특징 -->

### 🧞Genie
- 11B 개의 파라미터로 Foundation Model의 특성을 가집니다. 
- 20만 시간이 넘는 인터넷 게임 영상 데이터셋으로 훈련되었습니다.  
  - (필터링을 거쳐 정제된 데이터셋은 약 3만 시간)
- 액션 레이블이 없는 Video-only 데이터로 훈련되었음에도, Latent Action Space(잠재 액션 공간)를 학습해 프레임 단위의 액션 제어가 가능합니다. (중요)

## Genie의 구조

<!-- 세 가지 컴포넌트 -->
Genie는 크게 3 가지의 핵심 컴포넌트로 구성되어 있습니다. 

- **Latent Action Model(LAM)**: 프레임과 프레임 사이의 Latent Action을 학습합니다. 
- **Video Tokenizer**: Raw 비디오 프레임을 이산 토큰(Discrete Token)으로 변환합니다. 
- **Dynamics Model**: 이전 프레임들의 토큰과 Latent Action을 받아 다음 프레임을 예측합니다. 

### 0. ST-transformer

핵심 컴포넌트를 살펴보기에 앞서, 먼저 ST-transformer에 대해 알아야합니다. Genie의 모든 컴포넌트가 이 아키텍처를 채택했기 때문입니다. 

[ST-transformer](https://arxiv.org/pdf/2001.02908.pdf){:target="_blank"}는 Spatial-Temporal Transformer의 약자로, 공간적(Spatial) 정보와 시간적(Temporal) 정보를 함께 처리할 수 있는 트랜스포머 기반 모델입니다. 


<center><img width="505" alt="ST-transformer" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/cb6a3e41-a9d4-4d4f-8d11-f999e516b487">
</center>

<center><sup>ST-transformer 아키텍처</sup></center>  

ST-transformer는 Spatial attention 레이어, Temporal attention 레이어, Feed-forward 레이어로 구성된 L 개의  Spatiotemporal Block(시공간 블록) 으로 이루어져 있습니다. 

- Spatial Layer → 각 timestep마다 1 X H X W 토큰 연산
- Temporal Layer → T timestep에 대해 T X 1 X 1 토큰 연산  

이미지 처리에 많이 쓰이는 [ViT(Vision Transformer)](https://arxiv.org/abs/2010.11929){:target="_blank"}가 아닌, ST-transformer를 채택한 이유는 바로 높은 **메모리 효율성**입니다.

각 토큰이 시퀀스 내 다른 모든 토큰과 연결되는(Self-attention) 기존 Transformer 방식은 시간 레이어가 있는 비디오 데이터 특성상 연산 시 메모리 비용이 이차적으로(Quadratically) 증가해 효율성이 떨어진다는 문제가 있습니다. 

반면 ST-transformer는 연산 복잡도의 지배적 요인인 Spatial Layer의 수가 프레임 수에 비례해 선형적으로 증가하기 때문에 인터랙션이 증가해도 일관성을 유지하는 효율적인 비디오 생성이 가능합니다.  

### 1. Latent Action Model(LAM)

<center><img width="386" alt="LAM" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/d86c6693-bba9-4b8f-94b8-6069d0b521ec"></center>

<center><sup>Latent Action Model(LAM) 아키텍처</sup></center>

- **비지도 학습 방식:** 잠재 액션 모델은 프레임 사이의 잠재 액션(Latent Action) $a_t$를 비지도 학습(Unsupervised Learning)합니다. 액션 제어가 가능한 비디오 생성을 위해서는 이전 프레임에서 행해지는 액션에 대한 다음 프레임 예측을 레이블을 통해 조건화(Conditioning)해야 하는데, 액션 레이블은 인터넷에서 구하기 힘들고 어노테이션 비용이 많이 들기 때문입니다. 
- **Encoder 학습**  
  - 입력: 모든 이전 프레임 $x_{1:t}$ 와 다음 프레임 $x_{t+1}$ 
  - 출력: 대응되는 연속적인 Latent Actions $ \tilde{a}_{1:t} $ 
- **Decoder 학습**
  - 입력: 모든 이전 프레임 $x_{1:t}$ 와 Latent Actions $ \tilde{a}_{1:t} $ 
  - 출력: 예측한 다음 프레임 $\hat{x}_{t+1}$
- **ST-transformer 아키텍처 사용:** Temporal 레이어의 Causal Mask를 통해 전체 비디오 $x_{1:T}$를 입력으로 받아 각 프레임 사이 모든 Latent Actions $\tilde{a}_{1:T-1}$를 생성합니다. 

추론(Inference) 시에는 LAM 전체를 유저의 액션이 대체합니다. 


### 2. Video Tokenizer

<center><img width="437" alt="Video-tokenizer" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/8718e933-f836-46c8-8112-298d341e50f7"></center>

<center><sup>Video Tokenizer 아키텍처</sup></center>

- **VQ-VAE 방식**: 토크나이저는 전 비디오 시퀀스에 대해 Standard [VQ-VAE](https://arxiv.org/abs/1711.00937){:target="_blank"}를 사용하여 훈련됩니다. T 프레임으로 구성된  비디오 $x_{1:T} = (x_1, x_2, ... , x_T) \in \mathbb{R}^{T X HXWXC}$ 를 입력받아, 각 프레임에 대한 이산 표현 $z_{1:T} = (z_1, z_2, ... , z_T) \in \mathbb{I}^{TXD}$을 생성합니다. 
- **ST-transformer 아키텍처 사용:** 공간적인 압축에만 집중했던 기존 연구(Phenaki, MaskViT 등)와 달리, ST-transformer를 인코더와 디코더 모두에 사용해 인코딩 안의 Temporal Dynamics를 통합했고, 비디오 품질을 향상시켰습니다. 또한 ST-transformer의 특성에 따라, 각 이산 인코딩 $z_t$는 모든 이전 프레임 $x_{1:t}$의 정보를 포함합니다.  

Phenaki도 Temporal-aware 토크나이저(C-ViViT)를 사용하지만 Genie에서 사용한 ST-transformer 기반 토크나이저 (ST-ViVIT)가 훨씬 컴퓨팅 효율적이라는 것을 Ablation 실험 결과로 증명했습니다. 

<center><img width="452" alt="Tokenizer-ablation" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/50a525b6-06f0-4c7c-8fb7-bc0154b26c79"></center>

<center><sup>Tokenizer 아키텍처 Ablation</sup></center>


### 3. Dynamics Model

<center><img width="370" alt="Dynamics-model" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/00af5fa6-2884-4c81-9b64-a2eb354c2c78">
</center>
<center><sup>Dynamics Model 아키텍처</sup></center>

- **Decoder-only MaskGIT 구조:** [MaskGIT](https://arxiv.org/abs/2202.04200){:target="_blank"}의 Decoder를 사용합니다. 매 timestep t 마다 토큰화된 비디오 $z_{1:t-1}$와 잠재 액션 $\tilde{a}_{1:t-1}$을 입력받아 다음 프레임 토큰 $\hat{z}_t$을 예측합니다. 
- **ST-transformer 아키텍처 사용:** 아키텍처의 인과적(Causal) 구조로 인해 모든 (T-1)개의 프레임 $ z_{1:T-1} $의 토큰과 잠재 액션 $ \tilde{a}\_{1:T-1} $ 을 입력으로 사용할 수 있으며, 모든 다음 프레임 $ \hat{z}_{2:T} $에 대한 예측을 생성합니다.
- **Cross-entropy Loss:** 예측된 토큰 $ \tilde{z}\_{2:T} $ 와 Ground Truth 토큰 $z_{2:T}$ 사이의 크로스 엔트로피 손실 함수로 훈련됩니다. 

추론 시에는, 0.5~1.0 사이로 균일하게 샘플링된 베르누이 분포 마스킹 비율(Bernoulli Distribution Masking Rate)에 따라 입력 토큰 $z_{2:T-1}$을 무작위로 마스킹합니다. 

World Model을 훈련할 때 일반적인 관행은 시간 t에서의 액션을 해당 프레임과 연결시키는 것입니다. 그러나 연구진은 Latent Actions을 (Latent Action과 Dynamics Model 모두에 대한) 추가 임베딩(Additive Embeddings)으로 취급했고, 이것이 Controllability을 향상시킨다는 것을 발견했습니다. 

## Genie의 추론 과정

<center><img width="715" alt="Genie-inference" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/345ee735-0192-4438-a717-aa9e6e27240c"></center>

<center><sup> Genie의 추론 과정 </sup></center>

1. 플레이어가 모델에 이미지 $x_1$을 넣어 **프롬프팅** 한다. (→ 첫번째 프레임으로 기능)
2. 이미지가 비디오 인코더를 통해 $z_1$으로 **토큰화** 된다. 
3. 플레이어가 **잠재 액션** $a_1$ (\[1, |A|) 사이의 정수\)을 선택한다. 
4. Dynamics Model이 프레임 토큰 $z_1$과 대응되는 잠재 액션 $\tilde{a}\_1$ 을 받아 다음 프레임 토큰 $z_2$를 **예측**한다. 
5. 액션이 모델에 계속 입력됨에 따라 **Autoregressive** 방식으로 나머지 시퀀스 $\hat{z}\_{2:T}$를 생성한다. 
6. 동시에 토크나이저의 Decoder를 통해 토큰들이 비디오 프레임 $\hat{x}_{2:T}$로 **Decode**된다.


## 실험 결과

<!-- 데이터셋, Metric -->

### 확장 결과 (Scailing Results)

### 질적 결과 (Qualitative Results)


## 결론 및 향후 연구