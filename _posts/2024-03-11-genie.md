---
title: "Genie: Generative Interactive Environments 리뷰"
excerpt: "구글 딥마인드의 Foundation World Model, Genie 논문을 리뷰합니다."

categories: 
  - paper-review
# tags: 
#   - [tag1, tag2]

permalink: /paper-review/genie/ 

toc: true
toc_sticky: true
---

[https://arxiv.org/abs/2402.15391](https://arxiv.org/abs/2402.15391){:target="_blank"}  
[https://sites.google.com/view/genie-2024](https://sites.google.com/view/genie-2024){:target="_blank"}  

<img width="776" alt="genie_main" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/71f89e14-86a6-49b9-bb20-325d8aeb03b8">

## Intro

<!-- 간략한 소개 -->
지난 2월, Google Deepmind가 공개한 생성적 상호작용 환경 (Generative Interactive Environments) 모델, **Genie🧞** 입니다. Text-to-Image 모델로 생성한 이미지, 손으로 그린 스케치, 실제 사진을 입력으로 받아 액션 제어가 가능한 게임 세계(Playable Worlds)를 생성하는 모델입니다. 

<!-- 등장 배경 -->
지난 몇 년간 생성 AI의 엄청난 발전이 있었음에도, ChatGPT와 같은 언어 모델과 영상 생성 모델간의 사용자 상호작용 및 참여 수준의 격차는 여전히 존재합니다. 유저가 프롬프트를 입력하면 모델이 생성 결과를 출력하고, 그 결과를 바탕으로 유저가 또 다시 프롬프트를 입력하는 식의, 소위 '티키타카'라고 하는 상호작용이 생성 AI 사용에 있어 더욱 몰입감 있는 경험을 만들어내는 것도 사실입니다. 

*만일, 인터넷의 방대한 비디오 데이터가 주어진다면?*

새로운 이미지 및 비디오 생성 뿐만 아니라, 완전히 인터랙티브한 경험(Entire Interactive Experience)을 만들어낼 수 있지 않을까요? 이러한 맥락에서, 구글 딥마인드는 생성 AI의 새로운 패러다임, Generative Interactive Environments를 제안합니다. 하나의 텍스트/이미지 프롬프트로부터 인터랙티브한 '환경'을 생성하는 것이죠. 

<!-- 핵심 특징 -->

### 🧞Genie
- 11B 개의 파라미터로 Foundation Model의 특성을 가집니다. 
- 20만 시간이 넘는 인터넷 게임 영상 데이터셋으로 훈련되었습니다.  
  - (필터링을 거쳐 정제된 데이터셋은 약 3만 시간)
- 액션 레이블이 없는 Video-only 데이터로 훈련되었음에도, Latent Action Space(잠재 액션 공간)를 학습해 프레임 단위의 액션 제어가 가능합니다. (중요)

## Genie의 구조

<!-- 세 가지 컴포넌트 -->
Genie는 크게 3 가지의 핵심 컴포넌트로 구성되어 있습니다. 

- **Latent Action Model(LAM)**: 프레임과 프레임 사이의 Latent Action을 학습합니다. 
- **Video Tokenizer**: Raw 비디오 프레임을 이산 토큰(Discrete Token)으로 변환합니다. 
- **Dynamics Model**: 이전 프레임들의 토큰과 Latent Action을 받아 다음 프레임을 예측합니다. 

### 0. ST-transformer

핵심 컴포넌트를 살펴보기에 앞서, 먼저 ST-transformer에 대해 알아야합니다. Genie의 모든 컴포넌트가 이 아키텍처를 채택했기 때문입니다. 

[ST-transformer](https://arxiv.org/pdf/2001.02908.pdf){:target="_blank"}는 Spatial-Temporal Transformer의 약자로, 공간적(Spatial) 정보와 시간적(Temporal) 정보를 함께 처리할 수 있는 트랜스포머 기반 모델입니다. 


<center><img width="505" alt="ST-transformer" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/cb6a3e41-a9d4-4d4f-8d11-f999e516b487">
</center>

<center><sup>ST-transformer 아키텍처</sup></center>  

ST-transformer는 Spatial attention 레이어, Temporal attention 레이어, Feed-forward 레이어로 구성된 L 개의  Spatiotemporal Block(시공간 블록) 으로 이루어져 있습니다. 

- Spatial Layer → 각 timestep마다 1 X H X W 토큰 연산
- Temporal Layer → T timestep에 대해 T X 1 X 1 토큰 연산  

이미지 처리에 많이 쓰이는 [ViT(Vision Transformer)](https://arxiv.org/abs/2010.11929){:target="_blank"}가 아닌, ST-transformer를 채택한 이유는 바로 높은 **메모리 효율성**입니다.

각 토큰이 시퀀스 내 다른 모든 토큰과 연결되는(Self-attention) 기존 Transformer 방식은 시간 레이어가 있는 비디오 데이터 특성상 연산 시 메모리 비용이 이차적으로(Quadratically) 증가해 효율성이 떨어진다는 문제가 있습니다. 

반면 ST-transformer는 연산 복잡도의 지배적 요인인 Spatial Layer의 수가 프레임 수에 비례해 선형적으로 증가하기 때문에 인터랙션이 증가해도 일관성을 유지하는 효율적인 비디오 생성이 가능합니다.  

### 1. Latent Action Model(LAM)

<center><img width="386" alt="LAM" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/d86c6693-bba9-4b8f-94b8-6069d0b521ec"></center>
<center><sup>Latent Action Model(LAM)) 아키텍처</center></sup>



### 2. Video Tokenizer

<center><img width="437" alt="Video-tokenizer" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/8718e933-f836-46c8-8112-298d341e50f7"></center>
<center><sup>Video Tokenizer 아키텍처</center></sup>


### 3. Dynamics Model

<center><img width="370" alt="Dynamics-model" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/00af5fa6-2884-4c81-9b64-a2eb354c2c78">
</center>
<center><sup>Dynamics Model 아키텍처</center></sup>


## Genie의 추론 과정

## 실험 결과

<!-- 데이터셋, Metric -->

### 확장 결과 (Scailing Results)

### 질적 결과 (Qualitative Results)

### Ablation Studies

## 결론 및 향후 연구