---
title: "SAM 2: Segment Anything in Images and Videos 리뷰"
excerpt: "Segment Anything의 후속작, SAM2 논문을 리뷰합니다."

categories: 
  - paper-review
# tags: 
#   - [tag1, tag2]

permalink: /paper-review/sam2/ 

toc: true
toc_sticky: true
---

[https://arxiv.org/abs/2408.00714](https://arxiv.org/abs/2408.00714){:target="_blank"}  
[https://ai.meta.com/sam2/](https://ai.meta.com/sam2/){:target="_blank"}  

<img width="708" alt="sam2_main" src="https://github.com/user-attachments/assets/2c444957-4f5c-4f7f-9d40-b53491f4819e">

## Intro

<!-- 간략한 소개 -->
지난 2024년 8월 1일, SAM2가 등장했다. '프롬프트만 주면 뭐든 다 분할해 드립니다~'를 표방하며 등장했던 SAM(Segment Anything Model)이 나온 지 1년 4개월 만이다. 

기존 SAM과 비교했을 때, 단연 가장 큰 변화는 인풋의 도메인이다. 'Image' segmentation 모델에서, 'Image and Video' segmentation 모델로 그 범위를 확장했기 때문이다. 이미지는 실제 세계의 스냅샷에 불과하므로, 후속 모델에 있어 비디오로의 확장은 불가피했을 것이다. 

이미지와 비디오는 본질적으로 다르다. 이미지는 2차원, 비디오는 3차원. 비디오는 이미지 형식의 여러 프레임들을 시간 축을 기준으로 쌓아올린 것이다. 즉, 비디오 데이터에 대한 연산은 시간과 관련된 요소를 계산할 수 있는 모듈을 필요로 한다. 

SAM2는 비디오 데이터를 처리하기 위해 어떤 방법을 사용했을까? 비디오 데이터셋은 어떻게 수집했을까? 기존 SAM과 비교해 또 어떤 점이 달라졌을까? 함께 알아보자. 

<!-- 핵심 특징 -->

### 🧐 SAM2
- Promptable Visual Segmentation(PVS) 태스크를 제안
  - 이미지 세그멘테이션을 비디오 도메인으로 Generalize
- 비디오 데이터 연산을 위해 Memory Module을 새롭게 도입
- 기존 SAM보다 8.4배 빠른 자체적인 Data Engine 구축

## Video Segmentation


