---
title: "SAM 2: Segment Anything in Images and Videos 리뷰"
excerpt: "Segment Anything의 후속작, SAM2 논문을 리뷰합니다."

categories: 
  - paper-review
# tags: 
#   - [tag1, tag2]

permalink: /paper-review/sam2/ 

toc: true
toc_sticky: true
---

[https://arxiv.org/abs/2408.00714](https://arxiv.org/abs/2408.00714){:target="_blank"}  
[https://ai.meta.com/sam2/](https://ai.meta.com/sam2/){:target="_blank"}  

<img width="708" alt="sam2_main" src="https://github.com/user-attachments/assets/2c444957-4f5c-4f7f-9d40-b53491f4819e">

## Intro

<!-- 간략한 소개 -->
지난 2024년 8월 1일, SAM2가 등장했다. *'프롬프트만 주면 뭐든 다 분할해 드립니다~'* 를 표방하며 등장했던 SAM(Segment Anything Model)이 나온 지 1년 4개월 만이다. 

기존 SAM과 비교했을 때, 단연 가장 큰 변화는 인풋의 도메인이다. 'Image' segmentation 모델에서, 'Image and Video' segmentation 모델로 그 범위를 확장했기 때문이다. 이미지는 실제 세계의 스냅샷에 불과하므로, 후속 모델에 있어 비디오로의 확장은 불가피했을 것이다. 

이미지와 비디오는 본질적으로 다르다. 이미지는 2차원, 비디오는 3차원. 비디오는 이미지 형식의 여러 프레임들을 시간 축을 기준으로 쌓아올린 것이다. 즉, 비디오 데이터에 대한 연산은 시간과 관련된 요소를 계산할 수 있는 모듈을 필요로 한다. 

SAM2는 시간적 연산을 하기 위해 어떤 방법을 사용했을까? 비디오 데이터셋은 어떻게 수집했을까? 기존 SAM과 비교해 또 어떤 점이 달라졌을까? 함께 알아보자. 

---
<!-- 핵심 특징 -->

> ### 🧐 SAM2
> - Promptable Visual Segmentation **\(PVS) 태스크**를 제안
>   - 이미지 세그멘테이션을 비디오 도메인으로 Generalize
> - 비디오 데이터 연산을 위해 **Memory Module**을 새롭게 도입
> - 기존 SAM보다 8.4배 빠른 자체적인 **Data Engine** 구축

## Video Segmentation

비디오 세그멘테이션이란 무엇일까?  
논문에서는 어려운 말로 비디오 세그멘테이션의 목적을 *'개체의 시공간적 규모를 결정하는 것'* 이라고 말한다. 
이미지 세그멘테이션이 '이미지 내에서 관심 객체나 영역을 픽셀 단위로 구분짓는 작업'을 뜻한다는 것을 생각하면, 이를 비디오의 각 프레임별로 실행하는 것이라고 이해할 수 있다. 

비디오에 대해 세그멘테이션을 수행하는 일은 생각보다 어렵다. 저자는 크게 **3가지의 어려움**을 제시한다. 
1. 개체가 변할 수 있음 (움직임(motion), 변형(deformation), 가려짐(occlusion) 등의 이유로)
2. 영상의 퀄리티가 낮은 경우 많음 (카메라 움직임, 블러, 저해상도 등의 이유로)
3. 많은 프레임에 대한 효율적인 처리 어려움

SAM2는 이 어려움을 해결하고 이미지 및 비디오 세그멘테이션 계의 Foundation 모델을 만들기 위해 3가지를 제시했다. 
새로운 Task, Model, Data다. 

## Task: Promptable Visual Segmentation(PVS)
&nbsp;

**PVS 태스크**  
말 그대로 **Visual Data를 입력받아 Segmentation Mask를 출력**하는 태스크다. 특별한 점은 세그멘테이션 대상 및 범위를 사용자가 직접 지정할 수 있게끔 하는 **'프롬프팅'** 이 가능하다는 점이다. 인풋 비디오의 어떤 프레임이라도 프롬프트화 될 수 있으며, 프롬프트의 종류는 기존 SAM과 동일하다. (포인트, 마스크, 박스)
<center><img width="410" alt="sam2_task" src="https://github.com/user-attachments/assets/5c8e3b7f-666c-48cd-a963-59e093a81816"></center>

**Interactive Experience**  
프롬프팅이 가능하다는 것은 사용자와의 상호작용이 가능하다는 것이다. 기존 SAM과 마찬가지로, 인터랙티브한 사용자 경험을 제공하기 위해 노력했다.  
사용자가 프롬프트를 입력하면, 모델은 즉각적으로 해당 프레임의 유효한 Segmentation Mask를 출력한다. 이후 모델은 비디오 전체에 걸친 객체의 Masklet을 얻기 위해 이 초기 프롬프트를 Propagate한다. 모델의 출력 결과를 보면서, 사용자는 세그멘테이션을 개선하기 위해 언제라도 추가 프롬프트를 제공할 수 있다. 

> **Masklet**  
> 모든 비디오 프레임에 대한 대상 객체의 세그멘테이션 마스크로 구성



## Model


## Data
