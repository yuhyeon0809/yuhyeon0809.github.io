---
title: "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities 리뷰"
excerpt: "공간을 이해하는 VLM, 구글의 SpatialVLM을 리뷰합니다."

categories: 
  - paper-review
# tags: 
#   - [tag1, tag2]

permalink: /paper-review/spatialvlm/ 

toc: true
toc_sticky: true
---

[https://arxiv.org/abs/2401.12168](https://arxiv.org/abs/2401.12168){:target="_blank"}

<img width="655" alt="spatialvlm_main" src="https://github.com/user-attachments/assets/d50aa137-99b0-44a9-bf81-6cddbe103315">


## Intro
<!-- 간략한 소개 -->
고성능의 멀티모달 모델이 쏟아져 나오고 있는 요즘입니다. 최근 발표된 OpenAI o1부터 GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet까지, 멀티모달을 다루지 않고는 거대 모델 시장에서 살아남기 어려울 정도인데요. 텍스트-이미지 모달리티의 융합을 넘어서서 이제는 음성까지 섭렵해, 타이핑을 하지 않고도 AI와 음성 대화로 소통할 수 있게 됐습니다. 그리고 이를 가능케 한 주역 중의 하나로, VLM(Vision-Language Model)이 있습니다. 



## 1. Why Spatial Reasoning?
<!-- 공간적 추론의 필요성 -->

## 2. SpatialVLM
<!-- 핵심 특징(contribution) -->

### 1\) Spatial Grounding from 2D Images
<!-- 2D 이미지로부터 공간 추론을 위한 VQA 데이터를 생성하는 파이프라인 -->

### 2) Large-scale Spatial Reasoning VQA Datasets
<!-- 만들어진 VQA 데이터셋 -->

### 3) Learning Spatial Reasoning
<!-- 공간 추론을 어떻게 학습하는가? -->

## 3. Experiments
<!-- 3가지 질문 -->

### 1) Spatial VQA Performance
<!-- 양적/질적 성능 -->

### 2) Effect of Spatial VQA Data to General VQA
<!-- 일반적인 태스크의 성능 하락x -->

### 3) Effect of ViT Encoder in Spatial Reasoning
<!-- unfrozen ViT -->

### 4) Effect of Noisy Quantitative Spatial Answers

## Outro



