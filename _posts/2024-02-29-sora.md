---
title: "Sora: Technical Report 리뷰"
excerpt: "영상 생성 AI Sora의 기술 보고서를 간단히 리뷰합니다. "

categories: 
  - paper-review
# tags: 
#   - [tag1, tag2]

permalink: /paper-review/sora/ 

# toc: true
# toc_sticky: true
---

![](https://velog.velcdn.com/images/yuhyeon0809/post/2d9beb9a-b8c7-4098-86a3-79b1883b7002/image.mp4)

지난 2월 15일, ChatGPT로 유명한 OpenAI에서 영상 생성 AI **Sora**를 공개했다. 

최대 1분 가량의 초고품질 영상을 생성하는 Text-to-Video 모델로, 공개 직후부터 많은 화제가 되었다. ChatGPT의 첫 등장이 그랬던 것처럼, 또 한 번의 큰 파장이 일어날 전망이다.  

[관련 기사](https://www.aitimes.com/news/articleView.html?idxno=157296)와 [영상](https://www.youtube.com/watch?v=KEoE-XD_SiY&t=1s)이 쏟아져 나올 정도로 많은 분야에서 Sora에 관심을 보이는 이유는 바로 생성물의 퀄리티에 있다. AI가 만든 영상이라고는 믿기지 않을 정도로 사실적이며, 높은 완성도를 자랑한다.  

<details>
<summary>피식대학이 패러디한 Sora 영상 (재밌어서 공유)</summary>
<div markdown="1">

<iframe width="560" height="315" src="https://www.youtube.com/embed/s9WiifS8p0g?si=gVi9FTrENLJI2r6J" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>  

</div>
</details>  

---

현재 OpenAI는 Sora에 대한 [기술 보고서(Technical Report)](https://openai.com/research/video-generation-models-as-world-simulators)만 공개한 상태다. ***<Video generation models as world simulators\>*** 라는 제목의 기술 보고서를 읽고 간단히 정리해 보았다.  

[![sora 웹페이지](https://velog.velcdn.com/images/yuhyeon0809/post/83a479cd-88ad-4157-a342-6ec781bac517/image.png)](https://openai.com/sora)
<center><sub> 이미지를 누르면 Sora 웹페이지로 연결됩니다 </sub></center>

## Introduction

---

Sora는 다른 영상 생성 모델과 마찬가지로, 생성 모델을 대규모 영상 데이터로 훈련시키는 연구에서 비롯되었다. 다양한 지속 시간, 종횡비, 해상도를 가진 영상 및 이미지 데이터로 Text-conditional Diffusion 모델을 훈련시켰다.  

주목할만한 점은 Transformer 아키텍처를 사용했다는 점이다. Diffusion Transformer 구조로 샘플 퀄리티와 더불어 다양한 영역으로의 확장이 가능하다는데, 자세한 내용은 뒤에서 다룰 예정이다.  

### OpenAI의 Largest Model, Sora

Sora는 대규모의 영상 및 이미지 데이터로 훈련되었기 때문에, 현재 OpenAI의 모델 중 규모가 가장 큰 것으로 소개된다. 특히나 Runway, Pika, Stable Video Diffusion 등 타 영상 생성 AI 모델과 비교했을 때 Sora는 후발 주자이지만, 압도적인 성능으로 기존 모델과의 비교를 무의미하게 만들었다.  

이전 연구(Recurrent Network, GAN, Autoregressive Transformer 등)에서 Visual Data의 일부(짧은 비디오, 고정 크기 비디오 등)에 초점을 맞춘 경우가 다수 있었던 문제를 참고해, Sora는 다음과 같은 것들을 내세웠다. 

> ###### Sora가 내세운 것
> - Visual Data에 대한 Generalist Model(범용 모델)
> - 다양한 지속 시간, 해상도, 종횡비를 아우르는 최대 1분 분량의 고화질 영상 생성  

> ###### 본 기술 보고서(Technical Report)의 핵심
> - 모든 유형의 Visual Data를 (대규모 훈련을 위한) 통일된 표현(패치)으로 변환하는 방법
> - Sora의 기능 및 한계에 대한 질적 평가
> - 모델 및 세부 구현 정보는 포함하지 않음  

## Visual Data를 패치(Patch)로 표현

---

### 영감은 LLM으로부터  

ChatGPT를 개발한 LLM(대규모 언어 모델)의 명가 OpenAI 답게, Sora는 LLM으로부터 영감을 얻어 만들어졌다. 
연구진은 LLM 패러다임의 성공 요인을 다양한 양식의 텍스트(코드, 수식 등의 자연어)를 통합하는 **‘토큰’** 의 사용으로 보았고, '토큰' 역할을 하는 **'패치(Patches)'** 를 영상 생성 모델에 적용했다. 

### 패치란 무엇인가

Visual Data를 시간적/공간적으로 일정하게 분할한 것을 나타내는 개념이다.  

![](https://velog.velcdn.com/images/yuhyeon0809/post/58686766-c0c6-4c40-9942-9174da33f503/image.png)


영상을 패치로 분할하는 과정은 크게 두 단계를 거친다. 
1. 비디오를 낮은 차원의 Latent Space로 압축
2. 압축된 표현을 시공간(Spacetime) 패치로 분할  
   

> ###### 패치 기반 표현의 이점
> - Visual Data를 **효과적**으로 표현할 수 있다. 
> - 다양한 해상도, 지속 시간, 종횡비를 가진 비디오/이미지 데이터에 대한 **확장성**이 뛰어나다. 
> - 추론 결과로 생성된 영상의 **크기를 제어**할 수 있다.  
> -> 적절한 크기의 그리드에 랜덤 초기화된 패치를 배열함으로써

## 영상 생성으로 확장된 Diffusion Transformer

---

Sora는 **Diffusion Transformer** 구조다.  
기본적으로 **노이즈 패치**(및 텍스트 프롬프트 같은 Conditioning 정보)가 입력되면, 원래의 **‘clean’ 패치**를 예측하도록 훈련된다. 덧붙여 언어 모델링, 컴퓨터 비전, 이미지 생성 등 다양한 영역에서 놀라운 **확장 속성(Scailing Properties)** 을 보인다. 
![image](https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/56827acf-8ee1-45ae-aa66-63308e59e318)


### Diffusion Transformer란

Diffusion 모델의 Bockbone으로 U-Net이 아닌, Transformer 아키텍처를 적용한 모델이다. 줄여서 DiT라고 부르며, 일련의 패치에서 작동하는 [ViT(Vision Transformer)](https://arxiv.org/abs/2010.11929)를 기반으로 한다. 무엇보다 Transformer 아키텍처를 채택한 덕에 뛰어난 확장성을 지닌다는 것이 가장 큰 특징이다. [Diffusion Transformer 논문](https://arxiv.org/abs/2212.09748)을 참고하면 더 자세한 정보를 얻을 수 있다. 
<img width="707" alt="스크린샷 2024-03-02 오전 12 05 21" src="https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/301d2774-7af3-4698-b64d-89fbc912a512">
<center><sub>ViT(Vision Transformer)의 구조</sub></center>  

## Language Understanding 능력

---

Text-to-Video 생성 시스템을 학습시키기 위해서는 텍스트 캡션이 포함된 대량의 영상 데이터가 필요하다. Sora는 DALL-E 3에 도입된 **Re-Captioning 기술**을 영상 생성에 적용했는데, 1) 영상을 자세히 설명하는 Captioner 모델을 훈련한 다음, 2) 이를 활용해 훈련 세트의 모든 영상에 대한 캡션을 생성하는 식이다. 

![](https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/b7996543-a9a7-4fef-86db-c878e464caf8)

설명이 풍부한 캡션을 통한 학습은 결과물의 전반적인 품질은 물론, 텍스트 충실도(Fidelity)도 향상시키는 결과를 가져왔다. 결과적으로 사용자의 지시(Text Prompt)를 정확히 따르는 고품질 영상을 생성할 수 있게 된 것이다. 

## 그 밖의 Sora 기능들

---

이 외에도 많은 기능이 있다. 몇 가지만 간단히 소개하려 하는데, [Sora 웹페이지](https://openai.com/sora) 및 [기술 보고서](https://openai.com/research/video-generation-models-as-world-simulators)로 훑어보는 것을 추천한다. 

### Animating DALL·E images
![prompting_0](https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/416188d6-227f-4c62-825f-d52620473023) | ![dalle](https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/6eabbc4a-7932-43c8-9083-cc7ad3c8ec5e)
---|---|

DALL-E로 생성한 이미지를 입력으로 넣어 영상을 생성할 수 있다. 

### Extending generated videos

![bike](https://github.com/yuhyeon0809/yuhyeon0809/assets/98225529/ad2d3265-fdc2-4218-8412-7f923adc6a8e)

영상을 앞뒤로 확장해 매끄러운 무한루프를 생성할 수 있다. 

### Connecting videos

![a0](https://velog.velcdn.com/images/yuhyeon0809/post/5f704ffc-2c74-4301-affe-9c8957cb1934/image.mp4) | ![a1](https://velog.velcdn.com/images/yuhyeon0809/post/491b1b78-7e2f-4f18-9c13-dce1ff3dbd09/image.mp4) | ![a2](https://velog.velcdn.com/images/yuhyeon0809/post/b942dc72-f38a-4285-8176-cf7640d00a28/image.mp4)
---|---|---|

두 비디오 사이를 점진적으로 보간(Interpolate)해 완전히 다른 주제와 장면의 영상으로 매끄럽게 전환할 수 있다. (좌측 및 우측이 입력 영상, 가운데가 보간을 통해 생성된 영상이다)




































